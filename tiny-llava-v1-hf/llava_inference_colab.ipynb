{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# magic/dataset.py\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MAGICDataset(Dataset):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    :param _type_ Dataset: MAGICDataset for ImageCLEF 2024 challenge\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path:str=\"data/\", split:str=\"train\"):\n",
        "        \"\"\"\n",
        "        :param split: which dataset should be chosen\n",
        "        :param file_path: main path with data\n",
        "        \"\"\"\n",
        "        self.json_file = file_path + split + \"_downloaded.json\"\n",
        "        self.folder_path = file_path + \"images/\" + split\n",
        "        self.data = self._get_preprocessed_data()\n",
        "\n",
        "    def _get_preprocessed_data(self):\n",
        "        with open(self.json_file, encoding=\"utf8\") as f :\n",
        "            json_data = json.load(f)\n",
        "        temp_data = []\n",
        "        for sample in json_data:\n",
        "            if len(sample[\"image_ids\"]) != 1 :\n",
        "                logging.warning(f'Different number of images ({len(sample[\"image_ids\"])}) for question than 1')\n",
        "            image_path = self.folder_path + '/' + sample[\"image_ids\"][0] + '.jpg'\n",
        "            if not os.path.exists(image_path):\n",
        "                image_path = self.folder_path + '/' + sample[\"image_ids\"][0] + '.png'\n",
        "                if not os.path.exists(image_path):\n",
        "                    logging.warning(f\"Couldn't find path {image_path}\")\n",
        "                    continue\n",
        "            temp_data.append({\n",
        "                \"image\" : image_path,\n",
        "                \"description\" : sample[\"query_title_en\"],\n",
        "                \"answer\" : sample[\"responses\"][0][\"content_en\"],\n",
        "                \"encounter_id\" : sample[\"encounter_id\"]\n",
        "            })\n",
        "        return temp_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        prompt = (\n",
        "            \"This is additional information about the dermatology issue on the image:\"\n",
        "            + sample[\"description\"]\n",
        "            + \"What dermatological disease is on the image and how can it be treated?\"\n",
        "        )\n",
        "        return {\n",
        "            \"image\": Image.open(sample[\"image\"]),  # Should be a PIL image\n",
        "            \"qa\": [\n",
        "                {\n",
        "                    \"question\": prompt,\n",
        "                    \"answer\": sample[\"answer\"],\n",
        "                }\n",
        "            ], ## Why array?\n",
        "            \"encounter_id\": sample['encounter_id']\n",
        "        }"
      ],
      "metadata": {
        "id": "_i4ZhfnOdDgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzS9gprIc6zm"
      },
      "outputs": [],
      "source": [
        "# tiny-llava-v1-hf/inference.py\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "# from magic.dataset import MAGICDataset\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "#from llava.model import LlavaLlamaForCausalLM\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "print(\"START\")\n",
        "model_id = 'bczhou/tiny-llava-v1-hf'\n",
        "dataset = MAGICDataset(\"/content/drive/MyDrive/reddit/\", \"valid\")\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(model_id)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "response = []\n",
        "for sample in dataset:\n",
        "    text = f\"USER: <image>\\n {sample['qa'][0]['question']} ASSISTANT:\"\n",
        "    inputs = processor(text, sample['image'], return_tensors='pt')\n",
        "    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "    decoded_response = processor.decode(output[0][2:], skip_special_tokens=True)\n",
        "    print(decoded_response)\n",
        "    result = {\n",
        "        \"encounter_id\": sample[\"encounter_id\"],\n",
        "        \"responses\": [{\n",
        "            \"content_en\": decoded_response.split(\"ASSISTANT:\")[1]\n",
        "        }]\n",
        "    }\n",
        "    response.append(result)\n",
        "    with open('/content/drive/MyDrive/reddit/valid_data.json', 'w') as f:\n",
        "        json.dump(response, f)"
      ]
    }
  ]
}