{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXfLHJM6j6oU","executionInfo":{"status":"ok","timestamp":1715195132728,"user_tz":-120,"elapsed":6045,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}},"outputId":"f1a49b48-e2a1-48ce-8fee-81227f179acd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.16)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n","Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.5.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["%pip install torch transformers timm einops datasets bitsandbytes accelerate flash-attn"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4812,"status":"ok","timestamp":1715195137537,"user":{"displayName":"Patrycja","userId":"16357611780428678030"},"user_tz":-120},"id":"72QzU62gkoi9","outputId":"47737c94-8917-4625-bd89-38a8a822ae1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import json\n","import logging\n","import os\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fgTMEBsxkq_B","executionInfo":{"status":"ok","timestamp":1715195137537,"user_tz":-120,"elapsed":14,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["class MAGICDataset(Dataset):\n","    \"\"\"_summary_\n","\n","    :param _type_ Dataset: MAGICDataset for ImageCLEF 2024 challenge\n","    \"\"\"\n","    def __init__(self, split=\"train\"):\n","        self.json_file = \"data/\" + split + \"_downloaded.json\"\n","        self.folder_path = \"data/\" + split\n","        self.data = self._get_preprocessed_data()\n","\n","    def _get_preprocessed_data(self):\n","        with open(self.json_file, encoding=\"utf8\") as f :\n","            json_data = json.load(f)\n","        temp_data = []\n","        for sample in json_data:\n","            if len(sample[\"image_ids\"]) != 1 :\n","                logging.warning(f'Different number of images ({len(sample[\"image_ids\"])}) for question than 1')\n","            image_path = self.folder_path + '/' + sample[\"image_ids\"][0] + '.jpg'\n","            if not os.path.exists(image_path):\n","                image_path = self.folder_path + '/' + sample[\"image_ids\"][0] + '.png'\n","                if not os.path.exists(image_path):\n","                    logging.warning(f\"Couldn't find path {image_path}\")\n","                    continue\n","            query_content_en = '' if sample[\"query_content_en\"] in [\"[removed]\", \"[deleted]\"] else sample[\"query_content_en\"]\n","            temp_data.append({\n","                \"image\" : image_path,\n","                \"description\" : sample[\"query_title_en\"] + ';' + query_content_en,\n","                \"answer\" : sample[\"responses\"][0][\"content_en\"]\n","            })\n","        return temp_data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        prompt = (\n","            \"This is additional information about the dermatology issue on the image:\"\n","            + sample[\"description\"]\n","            + \"What dermatological disease is on the image and how can it be treated?\"\n","        )\n","        # prompt = (\n","        #     \"Patient wants to find out what dermatological disease he suffers from. Considering patient additional description:\"\n","        #     + sample[\"description\"] +\n","        #     \"Answer two questions: 1. What dermatological disease is on the image? 2. How can it be treated?\"\n","        # )\n","        return {\n","            \"image\": Image.open(sample[\"image\"]).convert('RGB'),  # Should be a PIL image\n","            \"qa\": [\n","                {\n","                    \"question\": prompt,\n","                    \"answer\": sample[\"answer\"],\n","                }\n","            ],\n","        }\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oaep8Cgyk9SG","executionInfo":{"status":"ok","timestamp":1715195137537,"user_tz":-120,"elapsed":13,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","from torch.utils.data import DataLoader\n","from bitsandbytes.optim import Adam8bit\n","import math\n","from einops import rearrange\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EgQ3IIoTkvd6","executionInfo":{"status":"ok","timestamp":1715195137538,"user_tz":-120,"elapsed":14,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["DEVICE = \"cuda\"\n","DTYPE = torch.float32 if DEVICE == \"cpu\" else torch.float16 # CPU doesn't support float16\n","MD_REVISION = \"2024-04-02\"\n","\n","# Number of tokens used to represent each image.\n","IMG_TOKENS = 729\n","ANSWER_EOS = \"<|endoftext|>\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"oAPgkArNvv12","executionInfo":{"status":"ok","timestamp":1715195137538,"user_tz":-120,"elapsed":14,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["from flash_attn import flash_attn_func, flash_attn_varlen_func\n","from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"L16DmrAvk2wM","executionInfo":{"status":"ok","timestamp":1715195137538,"user_tz":-120,"elapsed":14,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["class Finetune():\n","\n","    def __init__(self, train_dataset, valid_dataset, params):\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n","        self.model = AutoModelForCausalLM.from_pretrained(\n","                \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n","                attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else None,\n","                torch_dtype=DTYPE, device_map={\"\": DEVICE}\n","            )\n","        self.train_dataset = train_dataset\n","        self.valid_dataset = valid_dataset\n","        self.parameters = params\n","        self.augumentations = transforms.RandomOrder(\n","            [transforms.RandomRotation(10), transforms.RandomHorizontalFlip(0.5),transforms.ColorJitter(0.1, 0.1,0.1)]\n","            )\n","\n","\n","\n","    def _collate_fn(self, batch):\n","        images = [sample['image'] for sample in batch]\n","        # augumentation\n","        aug_images = []\n","        for image in images:\n","          aug_images.append(self.augumentations(image))\n","        images = aug_images\n","        images = torch.stack(self.model.vision_encoder.preprocess(images))\n","        images = rearrange(images,\n","                        \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\",\n","                        p1=14, p2=14)\n","\n","        labels_acc = []\n","        tokens_acc = []\n","\n","        for sample in batch:\n","            toks = [self.tokenizer.bos_token_id]\n","            labs = [-100] * (IMG_TOKENS + 1)\n","\n","            for qa in sample['qa']:\n","                q_t = self.tokenizer(\n","                    f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\",\n","                    add_special_tokens=False\n","                ).input_ids\n","                toks.extend(q_t)\n","                labs.extend([-100] * len(q_t))\n","\n","                a_t = self.tokenizer(\n","                    f\" {qa['answer']}{ANSWER_EOS}\",\n","                    add_special_tokens=False\n","                ).input_ids\n","                toks.extend(a_t)\n","                labs.extend(a_t)\n","\n","            tokens_acc.append(toks)\n","            labels_acc.append(labs)\n","\n","        max_len = -1\n","        for labels in labels_acc:\n","            max_len = max(max_len, len(labels))\n","\n","        attn_mask_acc = []\n","\n","        for i in range(len(batch)):\n","            len_i = len(labels_acc[i])\n","            pad_i = max_len - len_i\n","\n","            labels_acc[i].extend([-100] * pad_i)\n","            tokens_acc[i].extend([self.tokenizer.eos_token_id] * pad_i)\n","            attn_mask_acc.append([1] * len_i + [0] * pad_i)\n","\n","        return (\n","            images.to(dtype=DTYPE),\n","            torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n","            torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n","            torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n","        )\n","\n","    def _compute_loss(self, batch):\n","        images, tokens, labels, attn_mask = batch\n","\n","        images = images.to(DEVICE)\n","        tokens = tokens.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        attn_mask = attn_mask.to(DEVICE)\n","\n","        with torch.no_grad():\n","            img_embs = self.model.vision_encoder.encoder(images)\n","            img_embs = self.model.vision_encoder.projection(img_embs)\n","\n","        tok_embs = self.model.text_model.get_input_embeddings()(tokens)\n","        inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n","\n","        outputs = self.model.text_model(\n","            inputs_embeds=inputs_embeds,\n","            labels=labels,\n","            attention_mask=attn_mask,\n","        )\n","\n","        return outputs.loss\n","\n","    def _lr_schedule(self, step, max_steps):\n","        x = step / max_steps\n","        if x < 0.1:\n","            return 0.1 * self.parameters[\"learning_rate\"] + 0.9 * self.parameters[\"learning_rate\"] * x / 0.1\n","        else:\n","            return 0.1 * self.parameters[\"learning_rate\"] + 0.9 * self.parameters[\"learning_rate\"] * (1 + math.cos(math.pi * (x - 0.1))) / 2\n","\n","    def run(self):\n","\n","        dataloaders = {\n","            \"train\": DataLoader(\n","                self.train_dataset,\n","                batch_size=self.parameters['batch_size'],\n","                shuffle=True,\n","                collate_fn=self._collate_fn,\n","            ),\n","            \"val\": DataLoader(\n","                self.valid_dataset,\n","                batch_size=self.parameters['batch_size'],\n","                collate_fn=self._collate_fn,\n","            ),\n","        }\n","\n","        self.model.text_model.train()\n","        self.model.text_model.transformer.gradient_checkpointing_enable()\n","\n","        total_steps = self.parameters['epochs'] * len(dataloaders[\"train\"]) // self.parameters['grad_accum_steps']\n","        optimizer = Adam8bit(\n","            [\n","                {\"params\": self.model.text_model.parameters()},\n","            ],\n","            lr=self.parameters[\"learning_rate\"] * 0.1,\n","            betas=(0.9, 0.95),\n","            eps=1e-6\n","        )\n","\n","        i = 0\n","        for epoch in range(self.parameters['epochs']):\n","            for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{self.parameters['epochs']}\"):\n","                i += 1\n","\n","                loss = self._compute_loss(batch)\n","                loss.backward()\n","\n","                if i % self.parameters['grad_accum_steps'] == 0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    lr = self._lr_schedule(i / self.parameters['grad_accum_steps'], total_steps)\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","            # Calculate validation loss\n","            val_loss = 0\n","            for val_batch in tqdm(dataloaders[\"val\"], desc=\"Validation\"):\n","                with torch.no_grad():\n","                    val_loss += self._compute_loss(val_batch).item()\n","            val_loss /= len(dataloaders[\"val\"])\n","\n","            logs = {\n","                \"loss/train\": loss.item(),\n","                \"lr\": optimizer.param_groups[0]['lr'],\n","                \"loss/val\": val_loss\n","            }\n","            print(logs)\n","        # if USE_WANDB:\n","        #     wandb.finish()\n","        return self.model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"suIvtRzxAqB_","executionInfo":{"status":"ok","timestamp":1715195137538,"user_tz":-120,"elapsed":13,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466790,"status":"ok","timestamp":1715195604315,"user":{"displayName":"Patrycja","userId":"16357611780428678030"},"user_tz":-120},"id":"BEk-jGCalFgC","outputId":"6fb7e118-1796-4653-940a-dc20c5eeea8d"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Couldn't find path data/train/125gfgg.png\n","WARNING:root:Couldn't find path data/train/11m079w.png\n","WARNING:root:Couldn't find path data/train/11luysi.png\n","WARNING:root:Couldn't find path data/train/11pdmeh.png\n","WARNING:root:Couldn't find path data/train/120abe5.png\n","WARNING:root:Couldn't find path data/train/11kbxmx.png\n","WARNING:root:Couldn't find path data/train/11v4x0x.png\n","WARNING:root:Couldn't find path data/train/125745d.png\n","WARNING:root:Couldn't find path data/train/123um91.png\n","WARNING:root:Couldn't find path data/train/11orme3.png\n","WARNING:root:Couldn't find path data/train/11qzg8z.png\n","WARNING:root:Couldn't find path data/train/125rjud.png\n","WARNING:root:Couldn't find path data/train/11k1l42.png\n","WARNING:root:Couldn't find path data/train/11gjsy4.png\n","WARNING:root:Couldn't find path data/train/11hqjqp.png\n","WARNING:root:Couldn't find path data/train/11f66nu.png\n","WARNING:root:Couldn't find path data/train/11udo7q.png\n","WARNING:root:Couldn't find path data/train/121pxe0.png\n","WARNING:root:Couldn't find path data/train/124a8u3.png\n","WARNING:root:Couldn't find path data/train/11gdldk.png\n","WARNING:root:Couldn't find path data/train/11u3657.png\n","WARNING:root:Couldn't find path data/train/123xdrr.png\n","WARNING:root:Couldn't find path data/train/11u1n4p.png\n","WARNING:root:Couldn't find path data/train/10z03ls.png\n","WARNING:root:Couldn't find path data/train/11k4qp7.png\n","WARNING:root:Couldn't find path data/train/123aa44.png\n","WARNING:root:Couldn't find path data/train/121bwmr.png\n","WARNING:root:Couldn't find path data/train/11knbvy.png\n","WARNING:root:Couldn't find path data/train/11eo551.png\n","WARNING:root:Couldn't find path data/train/124zivv.png\n","WARNING:root:Couldn't find path data/train/11fvmz1.png\n","WARNING:root:Couldn't find path data/train/124wkox.png\n","WARNING:root:Couldn't find path data/train/11nzd94.png\n","WARNING:root:Couldn't find path data/train/11po7yz.png\n","WARNING:root:Couldn't find path data/train/11t12qh.png\n","WARNING:root:Couldn't find path data/train/11tvf7t.png\n","WARNING:root:Couldn't find path data/train/11ez9fd.png\n","WARNING:root:Couldn't find path data/train/127tn48.png\n","WARNING:root:Couldn't find path data/train/11psqch.png\n","WARNING:root:Couldn't find path data/train/11kcdqq.png\n","WARNING:root:Couldn't find path data/train/124ck6r.png\n","WARNING:root:Couldn't find path data/train/11pxemw.png\n","WARNING:root:Couldn't find path data/train/104l6j9.png\n","WARNING:root:Couldn't find path data/train/11i296a.png\n","WARNING:root:Couldn't find path data/train/123c02o.png\n","WARNING:root:Couldn't find path data/train/11fv382.png\n","WARNING:root:Couldn't find path data/train/11zweux.png\n","WARNING:root:Couldn't find path data/train/11ak1zg.png\n","WARNING:root:Couldn't find path data/train/ypv0f2.png\n","WARNING:root:Couldn't find path data/train/11unx8l.png\n","WARNING:root:Couldn't find path data/train/vpuwtx.png\n","WARNING:root:Couldn't find path data/train/1231vqb.png\n","WARNING:root:Couldn't find path data/train/11ywi97.png\n","WARNING:root:Couldn't find path data/train/11oqi7g.png\n","WARNING:root:Couldn't find path data/train/1249o6g.png\n","WARNING:root:Couldn't find path data/train/125rgon.png\n","WARNING:root:Couldn't find path data/train/11qexm1.png\n","WARNING:root:Couldn't find path data/train/11ruw54.png\n","WARNING:root:Couldn't find path data/train/11vqg9i.png\n","WARNING:root:Couldn't find path data/train/11stt07.png\n","WARNING:root:Couldn't find path data/train/11lm7d2.png\n","WARNING:root:Couldn't find path data/train/11tb3sz.png\n","WARNING:root:Couldn't find path data/train/102j8f6.png\n","WARNING:root:Couldn't find path data/train/11o1b5t.png\n","WARNING:root:Couldn't find path data/train/11iok0k.png\n","WARNING:root:Couldn't find path data/train/11zy1be.png\n","WARNING:root:Couldn't find path data/train/11t8aat.png\n","WARNING:root:Couldn't find path data/train/11sda8d.png\n","WARNING:root:Couldn't find path data/train/11xy898.png\n","WARNING:root:Couldn't find path data/train/1274ox7.png\n","WARNING:root:Couldn't find path data/train/11zojtu.png\n","WARNING:root:Couldn't find path data/train/tuyo2m.png\n","WARNING:root:Couldn't find path data/train/11fyt7o.png\n","WARNING:root:Couldn't find path data/train/xgbqnv.png\n","WARNING:root:Couldn't find path data/train/122hf4s.png\n","WARNING:root:Couldn't find path data/train/121tsxk.png\n","WARNING:root:Couldn't find path data/train/11tdh8d.png\n","WARNING:root:Couldn't find path data/valid/11k2tzj.png\n","WARNING:root:Couldn't find path data/valid/121olx7.png\n","WARNING:root:Couldn't find path data/valid/11hkjwl.png\n","WARNING:root:Couldn't find path data/valid/11q2v3y.png\n","WARNING:root:Couldn't find path data/valid/11nr7q9.png\n","WARNING:root:Couldn't find path data/valid/12656ip.png\n","WARNING:root:Couldn't find path data/valid/11jnh7e.png\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Epoch 1/2:   0%|          | 0/34 [00:00<?, ?it/s]WARNING:transformers_modules.vikhyatk.moondream2.9ba2958f5a886de83fa18a235d651295a05b4d13.modeling_phi:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","Epoch 1/2: 100%|██████████| 34/34 [03:28<00:00,  6.13s/it]\n","Validation:   0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Validation: 100%|██████████| 6/6 [00:22<00:00,  3.72s/it]\n"]},{"output_type":"stream","name":"stdout","text":["{'loss/train': 2.0971007347106934, 'lr': 2.0671729424061793e-05, 'loss/val': 1.9094409743944805}\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/2: 100%|██████████| 34/34 [03:26<00:00,  6.08s/it]\n","Validation: 100%|██████████| 6/6 [00:22<00:00,  3.72s/it]"]},{"output_type":"stream","name":"stdout","text":["{'loss/train': 1.4743281602859497, 'lr': 3.6607370300154272e-06, 'loss/val': 1.849326451619466}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["    parmas = {\n","        'grad_accum_steps' : 1,\n","        'batch_size' : 8,\n","        'epochs' : 2,\n","        'learning_rate' : 3e-5\n","    }\n","\n","    finetune = Finetune(train_dataset=MAGICDataset('train'), valid_dataset=MAGICDataset('valid'), params= parmas)\n","    model = finetune.run()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"C4ekCs73qPR3","executionInfo":{"status":"ok","timestamp":1715195616373,"user_tz":-120,"elapsed":12073,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["model.save_pretrained(\"checkpoints/moondream-3-1\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"_Iv6_oJOF9o5","executionInfo":{"status":"ok","timestamp":1715197185713,"user_tz":-120,"elapsed":11812,"user":{"displayName":"Patrycja","userId":"16357611780428678030"}}},"outputs":[],"source":["!cp -r './checkpoints/moondream-3-1' /content/gdrive/MyDrive/Exp1/"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP3sAvsx03JtJ5+FGxulTxH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}